# NeuralNetworkSolver

##### Even though Artificial Neural Network (ANN) is applied almost every-where nowadays, little do people know about its inner working principles. Deeper understandings can help make ANN more reliable, reduce its re-dundancy and expand its application on mobile devices. So far there are some studies about ANN’s inner work. Visualizations on ANN’s weights and activation functions are put up with. Occlusion experiments are de-signed to plot “interest degree” of Convolutional Neural Network (CNN) on various parts of an image in New York University.
##### This thesis put forward a new idea, which is to use numerical analysis to analyze the arithmetic logic of Backpropagation Neural Network (BPNN), whose activation function is Rectified Linear Unit (ReLU), and to visualize the network in a certain way. In this way, the difficulties in ana-lyzing ANN can be reduced. The main work includes:
##### First, Based on the characteristic of BPNN and ReLU, an automatic numerical analysis system has been designed. By dividing the system into modules and with the help of Mathematica, these modules have been de-veloped in Java.
##### Second, restricted by the performance limitation of the system, a BPNN has been constructed and trained. Then the system has analyzed the BPNN and visualized the result. With mathematical expressions and func-tional graphs, the original network could be analyzed somehow to dig into the mathematical meaning about how it processed the dataset.
##### Through experiments this thesis verified the system’s validity and achieved the analysis on the interior of ANN, which processed the dataset on linear function relationship. However, excessive complexity of the system limits its application. This defect will be resolved in future work.
